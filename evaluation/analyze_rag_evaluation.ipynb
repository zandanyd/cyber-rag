{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparison of RAG vs No-RAG Performance\n",
    "This experiment evaluates the effectiveness of incorporating Retrieval-Augmented Generation (RAG) in answering predefined cybersecurity questions from blog posts.\n",
    "\n",
    "**Experiment Setup:**\n",
    "\n",
    "1. With RAG: The model answers questions based on relevant passages retrieved from the blog (retrieval-augmented).\n",
    "\n",
    "2. Without RAG: The model answers using the full blog content directly, without retrieval or context filtering.\n",
    "\n",
    "3. Both setups were executed using the mistralai/mistral-small-3-1-24b-instruct-2503 model, a compact 7.3 billion parameter LLM.\n",
    "\n",
    "4. The dataset includes 35 blogs and 350 predefined questions, ensuring a consistent and fair evaluation across both methods.\n",
    "\n",
    "5. Answers were graded using meta-llama/llama-3-70b-instruct (70B parameters), assigning a grade from 0 (incorrect) to 1 (correct) with partial values indicating partly correct answers.\n",
    "\n",
    "6. The evaluation emphasizes answer correctness (not exact match to ground truth), using a WatsonX-based grading system.\n",
    "\n",
    "This comparison demonstrates whether small, efficient models like Mistral can benefit from RAG to improve performance and reduce the dependency on large, expensive LLMs.\n"
   ],
   "id": "56ba3a5cbffb9840"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Without RAG:\n",
    "Using the LLM directly on the full article without retrieval."
   ],
   "id": "5ecc88f7b2c6c962"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the evaluation file\n",
    "with open('/content/answers_without_rag.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract blog URL from each entry (if available)\n",
    "df['blog_url'] = df.get('article_url') if 'article_url' in df.columns else None\n",
    "if 'blog_url' not in df.columns:\n",
    "    df['blog_url'] = df['question'].map(lambda x: next((row['url'] for row in data if row['question'] == x), None))\n",
    "\n",
    "# Basic statistics\n",
    "num_questions = len(df)\n",
    "num_blogs = df['blog_url'].nunique()\n",
    "eval_counts = df['evaluation'].value_counts()\n",
    "percentages = df['evaluation'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Total unique blogs: {num_blogs}\")\n",
    "print(f\"Total questions evaluated: {num_questions}\\n\")\n",
    "\n",
    "# Overall average grade\n",
    "overall_avg_grade = df['grade'].mean()\n",
    "print(f\"Overall average grade: {overall_avg_grade:.3f}\\n\")\n",
    "\n",
    "print(\"Evaluation Breakdown:\")\n",
    "for label in eval_counts.index:\n",
    "    print(f\" - {label}: {eval_counts[label]} ({percentages[label]:.1f}%)\")\n",
    "\n",
    "# Show side-by-side answers\n",
    "df = df.rename(columns={'rag_answer': 'model_answer'})\n",
    "print(\"\\nSample comparison of RAG vs Ground Truth:\")\n",
    "display(df[['question', 'ground_truth_answer', 'model_answer', 'evaluation']].head(10))\n",
    "\n",
    "\n",
    "# Accuracy by question\n",
    "accuracy_per_question = df.groupby('question')['evaluation'].apply(\n",
    "    lambda x: (x == 'Correct').sum() / len(x)\n",
    ").reset_index().rename(columns={'evaluation': 'accuracy'})\n",
    "accuracy_per_question = accuracy_per_question.sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nAccuracy per question:\")\n",
    "display(accuracy_per_question)\n"
   ],
   "id": "b80c5f387fc491c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# With RAG:\n",
    "Using retrieval-augmented generation over the article content."
   ],
   "id": "117093f765acbec7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the evaluation file\n",
    "with open('/content/answers_with_rag.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract blog URL from each entry (if available)\n",
    "df['blog_url'] = df.get('article_url') if 'article_url' in df.columns else None\n",
    "if 'blog_url' not in df.columns:\n",
    "    df['blog_url'] = df['question'].map(lambda x: next((row['url'] for row in data if row['question'] == x), None))\n",
    "\n",
    "# Basic statistics\n",
    "num_questions = len(df)\n",
    "num_blogs = df['blog_url'].nunique()\n",
    "eval_counts = df['evaluation'].value_counts()\n",
    "percentages = df['evaluation'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Total unique blogs: {num_blogs}\")\n",
    "print(f\"Total questions evaluated: {num_questions}\\n\")\n",
    "\n",
    "# Overall average grade\n",
    "overall_avg_grade = df['grade'].mean()\n",
    "print(f\"Overall average grade: {overall_avg_grade:.3f}\\n\")\n",
    "\n",
    "print(\"Evaluation Breakdown:\")\n",
    "for label in eval_counts.index:\n",
    "    print(f\" - {label}: {eval_counts[label]} ({percentages[label]:.1f}%)\")\n",
    "\n",
    "# Show side-by-side answers\n",
    "print(\"\\nSample comparison of RAG vs Ground Truth:\")\n",
    "display(df[['question', 'ground_truth_answer', 'rag_answer', 'evaluation']].head(10))\n",
    "\n",
    "# Accuracy by question\n",
    "accuracy_per_question = df.groupby('question')['evaluation'].apply(\n",
    "    lambda x: (x == 'Correct').sum() / len(x)\n",
    ").reset_index().rename(columns={'evaluation': 'accuracy'})\n",
    "accuracy_per_question = accuracy_per_question.sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nAccuracy per question:\")\n",
    "display(accuracy_per_question)\n"
   ],
   "id": "7d0df5a9cb83a27c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG vs No-RAG Performance",
   "id": "6fb1963ff454a8c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# Load both evaluation files from local paths\n",
    "with open('/content/answers_with_rag.json', 'r', encoding='utf-8') as f:\n",
    "    with_rag = pd.DataFrame(json.load(f))\n",
    "\n",
    "with open('/content/answers_without_rag.json', 'r', encoding='utf-8') as f:\n",
    "    without_rag = pd.DataFrame(json.load(f))\n",
    "\n",
    "# Rename answer columns to distinguish\n",
    "with_rag = with_rag.rename(columns={'rag_answer': 'answer_with_rag'})\n",
    "without_rag = without_rag.rename(columns={'rag_answer': 'answer_without_rag'})\n",
    "\n",
    "# Merge on question\n",
    "merged_df = pd.merge(\n",
    "    with_rag[['question', 'answer_with_rag', 'evaluation', 'grade']],\n",
    "    without_rag[['question', 'answer_without_rag', 'evaluation', 'grade']],\n",
    "    on='question',\n",
    "    suffixes=('_with', '_without')\n",
    ").rename(columns={\n",
    "    'evaluation_with': 'eval_with_rag',\n",
    "    'evaluation_without': 'eval_without_rag'\n",
    "})\n",
    "\n",
    "# Display comparison table\n",
    "display(merged_df[['question', 'answer_without_rag', 'answer_with_rag']].head(10))\n",
    "\n",
    "# Bar plot comparing evaluation counts\n",
    "eval_counts = pd.DataFrame({\n",
    "    'Without RAG': without_rag['evaluation'].value_counts(),\n",
    "    'With RAG': with_rag['evaluation'].value_counts()\n",
    "}).fillna(0)\n",
    "\n",
    "# Reorder\n",
    "eval_counts = eval_counts.reindex(['Correct', 'Partially Correct', 'Incorrect'])\n",
    "\n",
    "# Plot\n",
    "eval_counts.plot(kind='bar', color=['lightcoral', 'mediumseagreen'], edgecolor='black')\n",
    "plt.title(\"Evaluation Results: With vs Without RAG\")\n",
    "plt.ylabel(\"Number of Answers\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "b51c166ffc555af"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
